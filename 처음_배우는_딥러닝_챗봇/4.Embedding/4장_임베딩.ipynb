{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4장_임베딩.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YncP32CTbDPz"
      },
      "source": [
        "## 4.1 임베딩이란? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm4K8O-gbLvZ"
      },
      "source": [
        "임베딩은 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정을 의미한다. 따라서 딥러닝 모델의 입력값으로 많이 사용된다. 말뭉치의 의미에 따라 벡터화되기 때문에 문법적인 정보가 포함되어 있다. 이 때문에 임베딩 품질이 좋다면 간단한 모델로도 높은 성능을 보일 수 있다.\r\n",
        "\r\n",
        "**<임베딩 기법>**\r\n",
        "\r\n",
        "* 문장 임베딩 : 문장 전체를 벡터화하는 방법, 문맥적 의미를 내포할 수 있다. 하지만 많은 문장 데이터가 필요하고, 학습 시 비용이 많이 들어간다.\r\n",
        "* 단어 임베딩 : 개별 단어를 벡터로 표현하는 방법, 문장 임베딩에 비해 학습 방법이 간단해 실무에서 여전히 많이 사용된다. 그러나 동음이의어에 대한 구분을 하지 않아 의미가 다르더라도 단어의 형태가 같다면 동일한 벡터값으로 표현된다.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfI5bMEccTx"
      },
      "source": [
        "## 4.2 단어 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krrxV6l_cgWe"
      },
      "source": [
        "### 4.2.1 원-핫 인코딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBs_zDRRcqaz"
      },
      "source": [
        "단어를 숫자 벡터로 변환하는 가장 기본적인 방법이다. 이름에서 알 수 있듯 요소들 중 하나의 값만 1을 가지고 나머지는 0을 갖는 인코딩을 의미한다. 전체 요소 중 단 하나의 값만 1이기에 희소(sparse) 벡터라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UiHx4PeeAwI"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_svDPQgYYeKi",
        "outputId": "63baee1b-8156-4a01-9310-bd8bfc266849"
      },
      "source": [
        "from konlpy.tag import Komoran\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "komoran = Komoran()\r\n",
        "text = \"오늘 하루는 정말 피곤하네요.\"\r\n",
        "\r\n",
        "nouns = komoran.nouns(text)\r\n",
        "print(nouns)\r\n",
        "\r\n",
        "dics = {}\r\n",
        "for word in nouns:\r\n",
        "  if word not in dics.keys():\r\n",
        "    dics[word] = len(dics)\r\n",
        "print(dics)\r\n",
        "\r\n",
        "# 원-핫 인코딩!\r\n",
        "nb_classes = len(dics)\r\n",
        "targets = list(dics.values())\r\n",
        "one_hot_targets = np.eye(nb_classes)[targets] # np.eye : 단위행렬을 만들어주는 함수\r\n",
        "print(one_hot_targets)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['오늘', '하루', '피곤']\n",
            "{'오늘': 0, '하루': 1, '피곤': 2}\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt_hyrvihTP6"
      },
      "source": [
        "### 4.2.2 희소 표현과 분산 표현 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_t7jNNjhX8n"
      },
      "source": [
        "희소 표현은 단어 사전의 길이가 길어지면 길어질 수록 메모리 낭비와 계산 복잡도가 커지는 단점이 있다. 또한 단어 간의 연관성이 전혀 없어 의미를 담을 수 없다. 또한 차원의 저주에 걸릴 가능성이 매우 높아진다. 이러한 문제를 해결하기 위해 고안된 방법은 분산 표현(distributed representation)이라고 한다. 분산 표현은 한 단어의 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현된다 하여 지어진 이름이다.\r\n",
        "   \r\n",
        "신경망이 분산 표현을 학습하는 과정에서 임베딩 벡터의 모든 차원에 의미있는 데이터를 고르게 밀집시킨다. 이 때문에 데이터 손실을 최소화하면서 벡터 차원이 압축되는 효과가 생긴다. 분산 표현 방식은 밀집 표현(dense representation)이라 부르기도 하면서, 밀집 표현으로 만들어진 벡터를 밀집 벡터(dense vector)라고 한다.\r\n",
        "\r\n",
        "**<분산 표현의 장점>**\r\n",
        "* 임베딩 벡터의 차원을 데이터 손실을 최소화하며 압축할 수 있다. 입력 데이터의 차원이 너무 높아질 경우, 신경망 모델의 학습이 어려워지는 차원의 저주 문제가 발생한다.\r\n",
        "* 임베딩 벡터에는 단어의 의미, 주변 단어간의 관계 등 많은 정보가 내포되어 있어 일반화 능력이 뛰어나다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJjWtj4rjxrd"
      },
      "source": [
        "### 4.2.3 Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP7eeBF8j3l9"
      },
      "source": [
        "챗봇의 경우 많은 단어를 처리하면서 단어 간 유사도를 계산할 수 있어야 좋은 성능을 낼 수 있기 때문에 원-핫 인코딩 기법은 좋은 선택이 아니다. 대표적인 신경망 단어 임베딩 방법인 Word2Vec을 소개하고 사용방법을 알아 보겠다. \r\n",
        "\r\n",
        "Word2Vec 모델은 CBOW, skip-gram 총 두가지 모델로 제안되었다. CBOW는 맥락(context word)라 불리는 주변 단어들을 이용해 타깃 단어를 예측하는 신경망 모델이다. 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해학습된 가중치 데이터를 임베딩 벡터로 활용한다. 타깃 단어를 예측하기 위해 앞 뒤로 몇 개의 단어를 확인할지 결정할 수 있는데, 이 범위를 윈도우(window)라고 한다.\r\n",
        "\r\n",
        "이와 반대로 skip-gram 모델은 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델이다. skip-gram이 CBOW에 비해 예측해야 하는 맥락이 많아 단어 분산 표현력이 우수해 임베딩 품질이 우수하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PKbN71sqUfi",
        "outputId": "67495088-9cd5-4d3a-d4ae-40fafed4541a"
      },
      "source": [
        "import urllib\r\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings_train.txt', <http.client.HTTPMessage at 0x7f0c039eea20>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObR2_92sjA_v",
        "outputId": "b05aa77d-f8de-4038-9ec1-8d9a1776e402"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "from konlpy.tag import Komoran\r\n",
        "\r\n",
        "# 네이버 영화 리뷰 데이터를 사용한다\r\n",
        "def read_review_data(filename):\r\n",
        "  with open(filename, 'r') as f:\r\n",
        "    data = [line.split('\\t') for line in f.read().splitlines()]\r\n",
        "    data = data[1:]\r\n",
        "  return data\r\n",
        "\r\n",
        "print('1) 말뭉치 데이터 읽기 시작')\r\n",
        "review_data = read_review_data('/content/ratings_train.txt')\r\n",
        "print(len(review_data))\r\n",
        "\r\n",
        "print('2) 형태소에서 명사 추출 시작')\r\n",
        "komoran = Komoran()\r\n",
        "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\r\n",
        "\r\n",
        "print('3) 모델 학습 시작')\r\n",
        "# sentences : 모델 학습에 필요한 데이터, size : 단어 임베딩 벡터 차원, window : 윈도우 크기\r\n",
        "# hs : 0(0이 아닌 경우 음수 샘플링 사용), 1(모델 학습에 softmax 사용), min_count : 단어 최소 빈도수 제한, sg : 0(CBOW), 1(skip_gram)\r\n",
        "model = Word2Vec(sentences = docs, size = 200, window = 4, hs = 1, min_count = 2, sg = 1)\r\n",
        "\r\n",
        "print('4) 학습된 모델 저장 시작')\r\n",
        "model.save('nvmc.model')\r\n",
        "\r\n",
        "print('corpus_count : ', model.corpus_count)\r\n",
        "print('corpus_total_words : ', model.corpus_total_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) 말뭉치 데이터 읽기 시작\n",
            "150000\n",
            "2) 형태소에서 명사 추출 시작\n",
            "3) 모델 학습 시작\n",
            "4) 학습된 모델 저장 시작\n",
            "corpus_count :  150000\n",
            "corpus_total_words :  806264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uulifgwRuHIU"
      },
      "source": [
        "**만들어진 모델 읽어와 실제로 단어 임베딘된 값과 벡터 공간상의 유사한 단어들을 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GornUzXquGhA",
        "outputId": "e323ff89-4aff-4d90-8079-774f695d0cde"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "model = Word2Vec.load('/content/nvmc.model')\r\n",
        "print('corpus_total_words : ', model.corpus_total_words)\r\n",
        "\r\n",
        "# '사랑'이라는 단어로 생성한 임베딩 벡터\r\n",
        "print('사랑 ; ', model.wv['사랑'])\r\n",
        "\r\n",
        "# 단어 유사도 계산\r\n",
        "print('일요일 = 월요일\\t', model.wv.similarity(w1 = '일요일', w2 = '월요일'))\r\n",
        "\r\n",
        "# 가장 유사한 단어 추출\r\n",
        "print(model.wv.most_similar('안성기', topn = 5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus_total_words :  806264\n",
            "사랑 ;  [-6.74988031e-02  6.00074530e-01 -7.62046650e-02  1.71618193e-01\n",
            "  6.58940449e-02 -8.51436406e-02 -1.42345220e-01  1.20324470e-01\n",
            "  4.26264144e-02 -3.90026659e-01 -8.63191579e-03 -1.17552891e-01\n",
            "  3.94470729e-02  1.78302869e-01 -9.89935473e-02  3.65931541e-02\n",
            "  3.63852948e-01 -2.61451825e-02 -2.89338231e-02  3.69054675e-02\n",
            " -3.21664006e-01 -1.01783007e-01  1.64149269e-01  1.14270300e-01\n",
            " -1.71412736e-01 -1.62363142e-01  1.15882218e-01  9.93937626e-02\n",
            "  1.31306961e-01  4.28856127e-02 -1.88654080e-01 -1.49762660e-01\n",
            "  3.61318231e-01 -1.10992528e-01  2.32460633e-01 -1.61209926e-01\n",
            " -8.91700089e-02  3.06289166e-01  7.72543252e-02 -1.90806195e-01\n",
            "  2.04794779e-01  2.32813329e-01 -3.26476455e-01  2.61789709e-01\n",
            "  2.06414431e-01  1.18425757e-01  3.20243448e-01 -1.01320200e-01\n",
            " -2.83871889e-01  3.31708521e-01 -1.05258331e-01 -2.26199880e-01\n",
            " -5.02921753e-02  3.78054142e-01  3.43502387e-02  5.07646836e-02\n",
            "  1.66489952e-03  4.20453101e-01 -1.66918606e-01  9.02442634e-02\n",
            "  1.41169459e-01  4.40968871e-02 -1.57361642e-01 -1.99548900e-01\n",
            "  2.47026876e-01 -1.93829741e-02 -1.20715559e-01 -1.65949743e-02\n",
            " -2.76007563e-01  1.89564779e-01  1.38809890e-01  2.94307262e-01\n",
            "  1.37161389e-01 -3.22761238e-01 -6.34454042e-02  8.39521661e-02\n",
            "  1.25188157e-01 -1.22713916e-01  4.45143767e-02  5.66515736e-02\n",
            "  4.46661830e-01 -3.65772769e-02  1.67814180e-01 -2.36837849e-01\n",
            "  6.62907809e-02 -4.27720025e-02  2.70901531e-01  3.99110198e-01\n",
            "  3.48371208e-01  1.80680886e-01 -2.90096831e-03  1.53923869e-01\n",
            " -4.75911461e-02  7.36127496e-02  7.99224377e-02  1.25237415e-02\n",
            "  5.12547232e-02 -6.54775202e-02  5.94516210e-02 -2.16593236e-01\n",
            " -1.19467206e-01 -8.96199569e-02  5.04396260e-01  5.36607578e-02\n",
            "  1.02895556e-03  7.98524916e-02 -3.61814678e-01 -1.28211558e-01\n",
            " -3.61022949e-01 -3.08965713e-01  3.14834356e-01  3.84529263e-01\n",
            " -1.55699970e-02  1.91114038e-01  2.26443395e-01 -1.60511225e-01\n",
            " -1.62085772e-01 -4.56307054e-01  9.90294814e-02  1.47901729e-01\n",
            " -1.15742153e-02  8.22398365e-02 -1.32865041e-01 -1.58700719e-01\n",
            " -2.11347371e-01 -6.62231892e-02 -7.91501105e-02  1.84331760e-01\n",
            " -4.52109762e-02  9.67024192e-02  3.83293107e-02 -7.09070861e-02\n",
            " -7.32200071e-02 -2.81318307e-01  4.26012754e-01 -1.52934387e-01\n",
            " -3.83403599e-01  1.15208343e-01 -1.25470236e-01  1.35782957e-01\n",
            " -1.73082367e-01  1.35113284e-01  1.90112576e-01 -1.59390077e-01\n",
            " -2.73490936e-01  1.87760338e-01  1.65384173e-01  1.54029578e-02\n",
            " -3.67237814e-02  5.00308990e-01  5.02083719e-01  2.46229898e-02\n",
            "  1.63713247e-01  2.43099153e-01  1.57191336e-01 -1.06369160e-01\n",
            "  2.32031837e-01  1.27307951e-01  1.66918322e-01  1.29557073e-01\n",
            " -9.55969393e-02  3.96259129e-03  1.00845799e-01  7.34435841e-02\n",
            "  7.39356577e-02  2.52890557e-01  4.79827747e-02  8.40847492e-02\n",
            " -4.48284268e-01 -8.28253338e-04 -1.30144894e-01 -7.07382038e-02\n",
            "  8.55168104e-02  1.81067556e-01  1.70912325e-01  1.22243620e-01\n",
            "  1.47852181e-02 -7.54314288e-02  2.48892069e-01  5.66313465e-05\n",
            "  4.45313573e-01 -8.24387893e-02  4.43566553e-02 -1.63585842e-01\n",
            " -6.58982098e-02 -7.84106366e-03  5.91990948e-02 -2.40367368e-01\n",
            "  5.29570132e-02  3.38923603e-01 -2.65753865e-01 -3.41345966e-01\n",
            " -3.16092491e-01 -1.11206010e-01 -1.34272575e-01 -1.45873368e-01\n",
            " -3.91538441e-01  8.91229361e-02 -1.47172704e-01  4.60035428e-02]\n",
            "일요일 = 월요일\t 0.69474244\n",
            "[('이은우', 0.7661565542221069), ('이태준', 0.764805793762207), ('고현정', 0.7594757676124573), ('이문식', 0.7541418075561523), ('보아', 0.74457848072052)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}