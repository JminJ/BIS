{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.Text_similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11wTe_zrY-gt"
      },
      "source": [
        "## 5.1 텍스트 유사도 개요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPzy3v7lZCXM"
      },
      "source": [
        "사람은 두 개의 문장이 유사하다는 것을 두 개의 문장에 동일한 단어나 의미상 비슷한 단어들이 얼마나 분포되어 있는지 직감적으로 파악해 인지한다. 컴퓨터 또한 이와 동일한 방법으로 두 문장간의 유사도를 계산할 수 있다.\r\n",
        "\r\n",
        "임베딩을 통해 각 단어들의 벡터를 구한 다음 벡터 간의 거리를 계산해 단어 간의 의미가 얼마나 유사한지 계산할 수 있다. 이는 문장 또한 마찬가지다(단어의 묶음이기에).\r\n",
        "\r\n",
        "이 책에서는 Q&A챗봇을 다룰 것이기에 챗봇 엔진에 입력되는 문장과 시스템에서 해당 주제의 답변과 연관되어 있는 질문이 얼마나 유사한지 계산할 수 있어야 적절한 답변을 출력할 수 있다. 이처럼 두 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화해야 한다.\r\n",
        "\r\n",
        "이전에 배운 Word2Vec은 인공 신경망을 활용했지만 이번에는 통계 기반 방식을 살펴볼 것이다. 시작하기 앞서 언제나 인공 신경망 방식이 성능이 최고로 좋은 것은 아니라는 점을 알아두면 좋겠다. 상황에 따라 통계적인 방식이 더 적절할 수 있고 챗봇의 주제에 따라 사용하면 챗봇 엔진 성능 향상에 도움이 될 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRIvKYlzafUZ"
      },
      "source": [
        "## 5.2 n-gram 유사도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5KyhoXraiSc"
      },
      "source": [
        "n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)을 의미한다. n-gram은 문장에서 n개의 단어를 토큰으로 사용하는데 이는 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법이다. \r\n",
        "\r\n",
        "서로 다른 문장을 n-gram으로 비교하면 단어의 출현 빈도에 기반한 유사도를 계산할 수 있으며 이를 통해 논문 인용이나 도용 정도를 조사할 수 있다.\r\n",
        "\r\n",
        "<예제>\r\n",
        "**1661년** **6월** **뉴턴**은 **선생님**의 **제안**으로 **트리니티**에 **입학**하였다.\r\n",
        "n = 1 : 1661년 / 6월 / 뉴턴 / 선생님 / 제안 / 트리니티 / 입학\r\n",
        "n = 2 : 166년 6월 / 6월 뉴턴 / 뉴턴 선생님 / 선생님 제안 / 제안 트리니티 / 트리니티 입학\r\n",
        "...\r\n",
        "\r\n",
        "n이 1인 경우를 1-gram 또는 유니그림, 2인 경우 2-gram 또는 바이그램, 3인 겅우 3-gram 또는 트라이그램이라 부르며 4 이상은 숫자만 앞쪽에 붙여 부른다.\r\n",
        "\r\n",
        "예제처럼 문장을 n-gram으로 토큰을 분리한 후 단어 문서 행렬을 만든다. 이후 두 문장을 서로 비교해 동일한 단어의 출현 빈도를 확률로 계산해 유사도를 구할 수 있다.\r\n",
        "\r\n",
        "*similarity = tf(A,B) / tokens(A)*\r\n",
        "\r\n",
        "tf는 두 문장 A와 B에서 동일한 토큰의 출현 빈도를 뜻하고, tokens는 해당 문장에서 전체 토큰(n-gram으로 분리된 단어) 수를 의미한다. similarity가 1에 가까울 수록 B가 A에 유사하다고 볼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObfbZl63eJQ-"
      },
      "source": [
        "## 2-gram 유사도 계산 예제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gubH5LTIf-G4",
        "outputId": "bbda3519-f8ce-418c-b041-21daeba72d2e"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, tweepy, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM6B8kjSYRCw",
        "outputId": "ea8ed6db-a0b4-40ab-9609-f4e28f10d03e"
      },
      "source": [
        "from konlpy.tag import Komoran\r\n",
        "\r\n",
        "# 어절 단위 n-gram\r\n",
        "def word_ngram(bow, num_gram):\r\n",
        "  text = tuple(bow) # komoran으로 noun만 추출될 것\r\n",
        "  ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\r\n",
        "  return tuple(ngrams)\r\n",
        "\r\n",
        "def similarity(doc1, doc2):\r\n",
        "  cnt = 0\r\n",
        "  for token in doc1:\r\n",
        "    if token in doc2:\r\n",
        "      cnt += 1\r\n",
        "\r\n",
        "  return cnt / len(doc1)\r\n",
        "\r\n",
        "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.'\r\n",
        "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.'\r\n",
        "sentence3 = '나는 맛있는 밥을 뉴턴 선생님과 함께 먹었다.'\r\n",
        "\r\n",
        "komoran = Komoran()\r\n",
        "# 각 문장의 noun 추출\r\n",
        "bow1 = komoran.nouns(sentence1)\r\n",
        "bow2 = komoran.nouns(sentence2)\r\n",
        "bow3 = komoran.nouns(sentence3)\r\n",
        "\r\n",
        "doc1 = word_ngram(bow1, 2)\r\n",
        "doc2 = word_ngram(bow2, 2)\r\n",
        "doc3 = word_ngram(bow3, 2)\r\n",
        "\r\n",
        "print(doc1)\r\n",
        "print(doc2)\r\n",
        "\r\n",
        "r1 = similarity(doc1, doc2)\r\n",
        "r2 = similarity(doc3, doc1)\r\n",
        "\r\n",
        "print(r1)\r\n",
        "print(r2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'), ('입학',))\n",
            "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'), ('입학',))\n",
            "0.6666666666666666\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B07a4_Ggdno"
      },
      "source": [
        "## 5.3 코사인 유사도"
      ]
    }
  ]
}